---
import ProjectDetailLayout from "../../layouts/ProjectDetailLayout.astro";

const metrics = [
    { label: "Latency", value: "41ms @ 512px" },
    { label: "Model Size", value: "14MB (INT8)" },
    { label: "Throughput", value: "24 FPS" },
    { label: "Scale", value: "50k MAU" },
];
---

<ProjectDetailLayout
    title="Real-time Style Transfer on Edge"
    description="Porting VGG-19 based style transfer models to mobile devices using NCNN and INT8 quantization."
    metrics={metrics}
    github="#"
    demo="#"
>
    <h2>Problem & Constraints</h2>
    <p>
        Neural style transfer typically requires heavy GPU compute (<code
            translate="no">VGG-19</code
        >
        backbone), making it infeasible for real-time applications on mobile devices.
        The goal was to enable
        <strong>real-time video stylization (24fps+)</strong> on mid-range Android
        devices (<code translate="no">Snapdragon 855</code> class) while maintaining
        perceptual quality closest to the original optimization-based method.
    </p>

    <h2>Approach</h2>
    <p>
        The solution involved a multi-stage optimization pipeline focused on
        model architecture and inference engine tuning:
    </p>
    <ul>
        <li>
            <strong>Architecture Distillation:</strong> Distilled a heavy <code
                translate="no">VGG-19</code
            > encoder into a lightweight <code translate="no"
                >MobileNetV3-Small</code
            > backbone using perceptual loss.
        </li>
        <li>
            <strong
                >Quantization-Aware Training (<code translate="no">QAT</code
                >):</strong
            > Fine-tuned the decoder with quantization-aware training to minimize
            accuracy drop when converting to <code translate="no">INT8</code>.
        </li>
        <li>
            <strong>Inference Engine Migration:</strong> Ported the <code
                translate="no">PyTorch</code
            > model to <code translate="no">NCNN</code> (Tencent's high-performance
            neural network framework) to leverage
            <code translate="no">Vulkan</code> compute shaders on Android.
        </li>
    </ul>

    <h2>Baselines & Evaluation</h2>
    <p>Compared against standard implementations on the same hardware:</p>

    <div class="overflow-x-auto">
        <table class="w-full text-left border-collapse my-6">
            <thead>
                <tr class="border-b border-white/10 text-text-primary">
                    <th class="py-2 px-4">Method</th>
                    <th class="py-2 px-4">Latency (ms)</th>
                    <th class="py-2 px-4">Model Size</th>
                    <th class="py-2 px-4">LPIPS (Lower is better)</th>
                </tr>
            </thead>
            <tbody>
                <tr class="border-b border-white/5">
                    <td class="py-2 px-4"
                        >Original <code translate="no">VGG-19</code> (<code
                            translate="no">PyTorch Mobile</code
                        >)</td
                    >
                    <td class="py-2 px-4">840ms</td>
                    <td class="py-2 px-4">500MB+</td>
                    <td class="py-2 px-4">0.0 (Ref)</td>
                </tr>
                <tr class="border-b border-white/5">
                    <td class="py-2 px-4"
                        >Pruned <code translate="no">VGG-16</code></td
                    >
                    <td class="py-2 px-4">320ms</td>
                    <td class="py-2 px-4">120MB</td>
                    <td class="py-2 px-4">0.04</td>
                </tr>
                <tr class="bg-accent-primary/5">
                    <td class="py-2 px-4 font-semibold text-accent-primary"
                        >Our Method (<code translate="no">NCNN</code> + <code
                            translate="no">INT8</code
                        >)</td
                    >
                    <td class="py-2 px-4 font-semibold text-accent-primary"
                        >41ms</td
                    >
                    <td class="py-2 px-4 font-semibold text-accent-primary"
                        >14MB</td
                    >
                    <td class="py-2 px-4 font-semibold text-accent-primary"
                        >0.09</td
                    >
                </tr>
            </tbody>
        </table>
    </div>

    <h2>Error Analysis & Failure Modes</h2>
    <p>To ensure production reliability, I analyzed the top failure modes:</p>
    <ul>
        <li>
            <strong>Temporal Flicker:</strong> Frame-to-frame inconsistency in video.
            Solved by introducing a temporal consistency loss ($L&#123;temp&#125;$)
            using optical flow warping during training.
        </li>
        <li>
            <strong>Color Drift:</strong>
            <code translate="no">INT8</code> quantization caused color shifts in low-contrast
            regions. Mitigated by using a separate scale factor for style features
            in the <code translate="no">AdaIN</code> layer.
        </li>
        <li>
            <strong>Thermal Throttling:</strong> Sustained 60fps usage caused device
            heating. Implemented an adaptive frame-skipping logic that throttles inference
            frequency based on battery temperature sensors.
        </li>
    </ul>

    <details open>
        <summary>Engineering Notes: Profiling Bottlenecks</summary>
        <p>
            Profiling with <code translate="no"
                >Android Studio CPU Profiler</code
            > revealed that 40% of inference time was spent on memory copying between
            texture and buffer memory. Switching to zero-copy <code
                translate="no">Vulkan</code
            > textures in <code translate="no">NCNN</code>
            reduced total end-to-end latency by ~15ms.
        </p>
    </details>

    <h2>Productionization</h2>
    <p>
        The model is served on-device (offline availability). A separate "Model
        Delivery Service" (<code translate="no">Python</code>/<code
            translate="no">FastAPI</code
        >) handles OTA updates:
    </p>
    <ul>
        <li>
            <strong>A/B Testing:</strong> Can roll out new model weights to 5% of
            users.
        </li>
        <li>
            <strong>Monitoring:</strong> Telemetry tracks P99 latency and crash rates
            (<code translate="no">Sentry</code>).
        </li>
        <li>
            <strong>Rollback:</strong> Client app keeps the previous stable model
            version and auto-reverts if crash rate spikes > 1%.
        </li>
    </ul>

    <h2>What I'd Do Next</h2>
    <ul>
        <li>
            Implement arbitrary style transfer (currently supports fixed 20
            styles).
        </li>
        <li>Explore 4-bit quantization for further size reduction.</li>
        <li>
            Port to <code translate="no">WebGPU</code> for browser-based support.
        </li>
    </ul>
</ProjectDetailLayout>
