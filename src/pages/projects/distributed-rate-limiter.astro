---
import ProjectDetailLayout from "../../layouts/ProjectDetailLayout.astro";

const metrics = [
    { label: "Latency", value: "p99 < 2ms" },
    { label: "Throughput", value: "100k QPS" },
    { label: "False Positives", value: "0%" },
    { label: "Availability", value: "99.99%" },
];
---

<ProjectDetailLayout
    title="Distributed API Rate Limiter"
    description="High-performance sliding window rate limiter designed for strict SLA enforcement across distributed microservices."
    metrics={metrics}
    github="#"
>
    <h2>Problem & Constraints</h2>
    <p>
        A multi-tenant SaaS API was suffering from "thundering herd" issues
        where misconfigured client retry loops would saturate backend capacity,
        affecting healthy tenants. The requirement was to enforce per-key rate
        limits with <strong>microsecond-level precision</strong> while adding negligible
        latency (&lt; 5ms) to the request path.
    </p>

    <h2>Approach: "Boring" Reliability</h2>
    <p>
        I prioritized simple, proven infrastructure components to minimize
        failure modes.
    </p>
    <ul>
        <li>
            <strong>Algorithm:</strong> Sliding Window Counter (Redis Sorted Sets).
            Chosen over Fixed Window to avoid "window edge" spikes where 2x traffic
            could slip through.
        </li>
        <li>
            <strong>Implementation:</strong> Encapsulated logic in a single <strong
                >Lua script</strong
            > executed atomically on Redis. This prevents "check-then-set" race conditions
            without the overhead of distributed locks.
        </li>
        <li>
            <strong>Fallback:</strong> Implemented a "fail-open" strategy. If Redis
            becomes unreachable, the sidecar allows traffic to pass to preserve availability
            over protection.
        </li>
    </ul>

    <h2>Scale Boundaries</h2>
    <p>Explicitly defined performance cliffs based on load testing:</p>
    <div class="overflow-x-auto">
        <table class="w-full text-left border-collapse my-6">
            <thead>
                <tr class="border-b border-white/10 text-text-primary">
                    <th class="py-2 px-4">Traffic Load</th>
                    <th class="py-2 px-4">System Behavior</th>
                </tr>
            </thead>
            <tbody>
                <tr class="border-b border-white/5">
                    <td class="py-2 px-4">&lt; 100k QPS</td>
                    <td class="py-2 px-4"
                        ><strong>Comfortable.</strong> p99 latency &lt; 2ms. Single
                        Redis node CPU &lt; 40%.</td
                    >
                </tr>
                <tr class="border-b border-white/5">
                    <td class="py-2 px-4">100k - 400k QPS</td>
                    <td class="py-2 px-4"
                        ><strong>Strained.</strong> Redis network I/O becomes saturated.
                        Latency jitter increases to 5-10ms.</td
                    >
                </tr>
                <tr class="bg-red-500/10 border-l-2 border-red-500">
                    <td class="py-2 px-4 font-semibold text-red-400"
                        >> 500k QPS</td
                    >
                    <td class="py-2 px-4 font-semibold text-red-400"
                        ><strong>Failure Point.</strong> Single Redis shard CPU saturates
                        (single-threaded).</td
                    >
                </tr>
            </tbody>
        </table>
    </div>
    <p>
        <strong>Next Scale Step:</strong> To scale beyond 500k QPS, I would implement
        <strong>client-side token buffering</strong> (local accumulation) to reduce
        network round-trips, or shard Redis by API key hash.
    </p>

    <h2>Postmortem: The "Hot Key" Incident</h2>
    <div class="glass-card p-6 bg-white/5 border-l-4 border-accent-secondary">
        <h3 class="!mt-0 !mb-4 text-accent-secondary">Incident #L4-2024-03</h3>
        <p>
            <strong>Something failed:</strong> A single customer deployed a bug causing
            a tight loop of 50k requests/sec. Because all logic ran on one Redis primary,
            this "hot key" starved resource availability for other tenants.
        </p>
        <p>
            <strong>What I learned:</strong> Lack of Tier 1 (local memory) caching
            meant every request hit Redis, even for keys that were already blocked
            in the previous millisecond. Distributed systems need local protection.
        </p>
        <p>
            <strong>What I changed:</strong> Added a short-lived (1 second) local
            cache using
            <code>Ristretto</code> (Go) on the API Gateway to reject blocked keys
            at the edge.
        </p>
        <ul class="!mb-0">
            <li>
                If a key is blocked by Redis, cache <code>BLOCKED</code> status locally.
            </li>
            <li>
                Subsequent requests are rejected at the edge (0ms latency, 0
                Redis load).
            </li>
        </ul>
    </div>

    <details>
        <summary>Engineering Notes: Write vs Read Heavy</summary>
        <p>
            Rate limiting is a <strong>write-heavy</strong> workload (every check
            is a write/update). Optimizing for write throughput meant disabling expensive
            Redis durability features (AOF fsync every sec) since rate limit data
            is ephemeral and data loss is acceptable.
        </p>
    </details>

    <h2>What I'd Do Next</h2>
    <ul>
        <li>
            Implement Global Rate Limiting using CRDTs (Redis Enterprise
            active-active) for multi-region support.
        </li>
        <li>
            Add adaptive concurrency limits (Little's Law) based on backend
            latency signals.
        </li>
    </ul>
</ProjectDetailLayout>
